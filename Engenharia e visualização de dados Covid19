{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projeto final Spark - Engenharia e visualização de dados Covid19 de 2020 e 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Envio de dados para HDFS\n",
    "\n",
    "Os dados foram baixados no windows, enviados para a pasta input disponível no no namenode:/input com o comando:\n",
    "docker cp /mnt/c/Users/razer/OneDrive/'Área de Trabalho'/'Big data engineer - Semantix'/data-covid/ namenode:/input\n",
    "\n",
    "Com os dados no container do namenode, via bash do namenode foi enviado os docs da pasta namenode:/input/data-covid para o hdfs utilizando o comando put:\n",
    "\n",
    "hdfs dfs -put /input/data-covid/* /data-covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /data-covid # Os dados se encontram no hdfs na pasta data-covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /data-covid/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv #visualização dos dados .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Otimizar todos os dados do hdfs para uma tabela Hive particionada por município."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Visualização e engenharia de dados - Covid 19\")\\\n",
    "    .config(\"spark.sql.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sql('set hive.exec.dynamic.partition=true;')\n",
    "spark.sql('set hive.exec.dynamic.partition.mode=nonstrict;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "|regiao|estado|municipio|coduf|codmun|codRegiaoSaude|nomeRegiaoSaude|               data|semanaEpi|populacaoTCU2019|casosAcumulado|casosNovos|obitosAcumulado|obitosNovos|Recuperadosnovos|emAcompanhamentoNovos|interior/metropolitana|\n",
      "+------+------+---------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-25 00:00:00|        9|       210147125|             0|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-26 00:00:00|        9|       210147125|             1|         1|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-27 00:00:00|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-28 00:00:00|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-29 00:00:00|        9|       210147125|             2|         1|              0|          0|            null|                 null|                  null|\n",
      "+------+------+---------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid19_df = spark.read.csv(path=\"/data-covid/*.csv\", header=True, sep=\";\", inferSchema=True, ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)\n",
    "covid19_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar sessão Spark com Hive support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------+\n",
      "|  regiao|estado|coduf|codmun|codRegiaoSaude|nomeRegiaoSaude|               data|semanaEpi|populacaoTCU2019|casosAcumulado|casosNovos|obitosAcumulado|obitosNovos|Recuperadosnovos|emAcompanhamentoNovos|interior/metropolitana|municipio|\n",
      "+--------+------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------+\n",
      "|  Brasil|  null|   76|  null|          null|           null|2021-01-01 00:00:00|       53|       210147125|       7700578|     24605|         195411|        462|         6756284|               748883|                  null|     null|\n",
      "|Nordeste|    PI|   22|220000|          null|           null|2021-01-01 00:00:00|       53|            null|             0|         0|              0|          0|            null|                 null|                  null|     null|\n",
      "|  Brasil|  null|   76|  null|          null|           null|2021-01-02 00:00:00|       53|       210147125|       7716405|     15827|         195725|        314|         6769420|               751260|                  null|     null|\n",
      "|Nordeste|    PI|   22|220000|          null|           null|2021-01-02 00:00:00|       53|            null|             0|         0|              0|          0|            null|                 null|                  null|     null|\n",
      "|  Brasil|  null|   76|  null|          null|           null|2021-01-03 00:00:00|        1|       210147125|       7733746|     17341|         196018|        293|         6813008|               724720|                  null|     null|\n",
      "+--------+------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS covid19\")\n",
    "covid19_df.write\\\n",
    "    .option(\"header\", True)\\\n",
    "    .partitionBy(\"municipio\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .saveAsTable(\"covid19.geral\", path=\"/data-covid/tab_covid19\")\n",
    "\n",
    "spark.sql(\"USE covid19\")\n",
    "spark.sql(\"SELECT * FROM geral\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Criar visualizações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construção das queries para montar as visualizações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###Recuperados - Queries de referência para montar as queries para a visualização (recuperados_tab, casos_tab e obitos_tab.\n",
    "#casos_recuperados_tab = spark.sql(\"SELECT Recuperadosnovos, data FROM casos WHERE regiao='Brasil'\") #max\n",
    "#andamento_tab = spark.sql(\"SELECT emAcompanhamentoNovos FROM casos WHERE regiao ='Brasil'\") #last\n",
    "\n",
    "recuperados_tab = spark.sql(\"SELECT Recuperadosnovos, emAcompanhamentoNovos, data FROM geral WHERE regiao ='Brasil' ORDER BY data\")\n",
    "\n",
    "###Casos confirmados\n",
    "#casos_acumulados_tab = spark.sql(\"SELECT casosAcumulado, data FROM casos WHERE regiao ='Brasil'\") #max\n",
    "#novos_casos_tab = spark.sql(\"SELECT casosNovos, data FROM casos WHERE regiao ='Brasil'\") #last\n",
    "#incidencia_tab = spark.sql(\"SELECT casosNovos, populacaoTCU2019, data FROM casos WHERE regiao ='Brasil'\") # Incidencia = casosAcumulado/(populacaoTCU2019/ 100 000)\n",
    "\n",
    "casos_tab = spark.sql(\"SELECT casosAcumulado, casosNovos, populacaoTCU2019, data FROM geral WHERE regiao ='Brasil' ORDER BY data\")\n",
    "\n",
    "###Óbitos confirmados\n",
    "#obitos_acumulados_tab = spark.sql(\"SELECT obitosAcumulado, data FROM casos WHERE regiao='Brasil'\") #max\n",
    "#obitos_novos_tab = spark.sql(\"SELECT obitosNovos, data FROM casos WHERE regiao ='Brasil'\") # last\n",
    "#taxa_letalidade_tab = spark.sql(\"SELECT obitosAcumulado, emAcompanhamentoNovos, data FROM casos WHERE regiao ='Brasil'\")#obitoAcumulado/casosAcumulado\n",
    "#taxa_mortalidade_tab = spark.sql(\"SELECT populacaoTCU2019, obitosAcumulado,  data FROM casos WHERE regiao ='Brasil' \")#obitosAcumulado/populacaoTCU2019/100000\n",
    "\n",
    "obitos_tab = spark.sql(\"SELECT emAcompanhamentoNovos, obitosNovos, obitosAcumulado, populacaoTCU2019, data FROM geral WHERE regiao ='Brasil' ORDER BY data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquisição dos valores de interesse para as visualizações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Célula com alto processamento, devido aos collects desta célula, o consumo de potência do disco e CPU é alto,\n",
    "### Por se tratar de um dataset pequeno, é possível utilizar collect para consumir os dados, não é indicado utilizá-lo\n",
    "### com dataset grandes.\n",
    "\n",
    "#Visualização 1\n",
    "casos_recuperados = recuperados_tab.groupBy().max('Recuperadosnovos').collect()[0][0]\n",
    "casos_andamento = recuperados_tab.select(last('emAcompanhamentoNovos')).collect()[0][0]\n",
    "\n",
    "#Visualização 2\n",
    "casos_acumulados = casos_tab.groupBy().max('casosAcumulado').collect()[0][0]\n",
    "novos_casos = casos_tab.select(last('casosNovos')).collect()[0][0]\n",
    "popTCU2019 = casos_tab.groupBy().max('populacaoTCU2019').collect()[0][0]\n",
    "incidencia = \"%.1f\" %(int(casos_acumulados) / (popTCU2019 / 100000))\n",
    "\n",
    "#Visualização 3\n",
    "obitos_acumulado = obitos_tab.groupBy().max('obitosAcumulado').collect()[0][0]\n",
    "obitos_novos = obitos_tab.select(last('obitosNovos')).collect()[0][0]\n",
    "taxa_letalidade = \"%.1f\" %((100 * int(obitos_acumulado) / casos_acumulados))\n",
    "taxa_mortalidade = \"%.1f\" %(int(obitos_acumulado) / (popTCU2019 / 100000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualização 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Casos Recuperados\\n\", casos_recuperados,\"\\n\\nEm Acompanhamento\\n\", casos_andamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualização 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CASOS CONFIRMADOS\\n\")\n",
    "print(casos_acumulados,\"\\t\",novos_casos, \"\\n\")\n",
    "print(\"Acumulado\", \"\\t\", \"Casos novos\\n\")\n",
    "print(incidencia,\"\\n\")\n",
    "print(\"Incidência\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualização 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ÓBITOS CONFIRMADOS\\n\")\n",
    "print(obitos_acumulado,\"\\t\\t\\t\",obitos_novos, \"\\n\")\n",
    "print(\"Óbitos acumulados\", \"\\t\", \"Casos novos\\n\")\n",
    "print(taxa_letalidade,\"\\t\\t\\t\", taxa_mortalidade, \"\\n\")\n",
    "print(\"Letalidade\", \"\\t\\t\", \"Mortalidade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Salvar primeira visualização como tabela hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recuperados_tab.write\\\n",
    "    .option(\"header\", True)\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .saveAsTable(\"covid19.recuperados\", path=\"/data-covid/tab_recuperados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - Salvar a segunda visualização com formato parquet e compressão snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_tab.write\\\n",
    "    .option(\"header\", True)\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"compression\", \"snappy\")\\\n",
    "    .parquet(\"/data-covid/casos_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 - Salvar a terceira visualização em um tópico no Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kafka-topics.sh --bootstrap-server kafka:9092 --topic topic-spark-covid --create --partitions 1 --replication\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obitos_tab.write\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\\\n",
    "    .option(\"topic\",\"topic-spark-covid\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 - Criar visualização de dados com drill down de regiões (Sul,  Centro-Oeste, Norte, Nordeste e Sudeste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUL (Os agrupamentos via comando SQL demonstram tempos de processamento menores do que processamento por groupBy())\n",
    "casos_acumulados_s = spark.sql(\"SELECT MAX(casosAcumulado), estado FROM geral WHERE regiao ='Sul' GROUP BY estado\")\\\n",
    "    .groupBy().\\\n",
    "    sum().\\\n",
    "    collect()[0][0]\n",
    "\n",
    "popTCU2019_s = spark.sql(\"SELECT MAX(populacaoTCU2019), estado FROM geral WHERE regiao ='Sul' GROUP BY estado\")\\\n",
    "    .groupBy()\\\n",
    "    .sum()\\\n",
    "    .collect()[0][0]\n",
    "\n",
    "incidencia_s = \"%.1f\" %(int(casos_acumulados_s) / (popTCU2019_s / 100000))\n",
    "    \n",
    "obitos_acumulado_s = spark.sql(\"SELECT MAX(obitosAcumulado), estado FROM geral WHERE regiao ='Sul' GROUP BY estado\")\\\n",
    "    .groupBy()\\\n",
    "    .sum()\\\n",
    "    .collect()[0][0]\n",
    "\n",
    "taxa_mortalidade_s = \"%.1f\" %(int(obitos_acumulado_s) / (popTCU2019_s / 100000))\n",
    "\n",
    "atualizacao_s = spark.sql(\"SELECT MAX(data) FROM geral WHERE regiao ='Sul'\")\\\n",
    "    .collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Centro-Oeste\n",
    "casos_acumulados_co = spark.sql(\"SELECT MAX(casosAcumulado), estado FROM geral WHERE regiao ='Centro-Oeste' GROUP BY estado\")\\\n",
    "    .groupBy().\\\n",
    "    sum().\\\n",
    "    collect()[0][0]\n",
    "\n",
    "popTCU2019_co = spark.sql(\"SELECT MAX(populacaoTCU2019), estado FROM geral WHERE regiao ='Centro-Oeste' GROUP BY estado\")\\\n",
    "    .groupBy()\\\n",
    "    .sum()\\\n",
    "    .collect()[0][0]\n",
    "\n",
    "incidencia_co = \"%.1f\" %(int(casos_acumulados_co) / (popTCU2019_co / 100000))\n",
    "    \n",
    "obitos_acumulado_co = spark.sql(\"SELECT MAX(obitosAcumulado), estado FROM geral WHERE regiao ='Centro-Oeste' GROUP BY estado\")\\\n",
    "    .groupBy()\\\n",
    "    .sum()\\\n",
    "    .collect()[0][0]\n",
    "\n",
    "taxa_mortalidade_co = \"%.1f\" %(int(obitos_acumulado_co) / (popTCU2019_co / 100000))\n",
    "\n",
    "atualizacao_co = spark.sql(\"SELECT MAX(data) FROM geral WHERE regiao ='Centro-Oeste'\")\\\n",
    "    .collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Norte\n",
    "casos_acumulados_n = spark.sql(\"SELECT MAX(casosAcumulado), estado FROM geral WHERE regiao ='Norte' GROUP BY estado\")\\\n",
    "    .groupBy().\\\n",
    "    sum().\\\n",
    "    collect()[0][0]\n",
    "\n",
    "popTCU2019_n = spark.sql(\"SELECT MAX(populacaoTCU2019), estado FROM geral WHERE regiao ='Norte' GROUP BY estado\")\\\n",
    "    .groupBy()\\\n",
    "    .sum()\\\n",
    "    .collect()[0][0]\n",
    "\n",
    "incidencia_n = \"%.1f\" %(int(casos_acumulados_n) / (popTCU2019_n / 100000))\n",
    "    \n",
    "obitos_acumulado_n = spark.sql(\"SELECT MAX(obitosAcumulado), estado FROM geral WHERE regiao ='Norte' GROUP BY estado\")\\\n",
    "    .groupBy()\\\n",
    "    .sum()\\\n",
    "    .collect()[0][0]\n",
    "\n",
    "taxa_mortalidade_n = \"%.1f\" %(int(obitos_acumulado_n) / (popTCU2019_n / 100000))\n",
    "\n",
    "atualizacao_n = spark.sql(\"SELECT MAX(data) FROM geral WHERE regiao ='Norte'\")\\\n",
    "    .collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nordeste\n",
    "casos_acumulados_ne = spark.sql(\"SELECT MAX(casosAcumulado), estado FROM geral WHERE regiao ='Nordeste' GROUP BY estado\")\\\n",
    "    .groupBy().\\\n",
    "    sum().\\\n",
    "    collect()[0][0]\n",
    "\n",
    "popTCU2019_ne = spark.sql(\"SELECT MAX(populacaoTCU2019), estado FROM geral WHERE regiao ='Nordeste' GROUP BY estado\")\\\n",
    "    .groupBy()\\\n",
    "    .sum()\\\n",
    "    .collect()[0][0]\n",
    "\n",
    "incidencia_ne = \"%.1f\" %(int(casos_acumulados_ne) / (popTCU2019_ne / 100000))\n",
    "    \n",
    "obitos_acumulado_ne = spark.sql(\"SELECT MAX(obitosAcumulado), estado FROM geral WHERE regiao ='Nordeste' GROUP BY estado\")\\\n",
    "    .groupBy()\\\n",
    "    .sum()\\\n",
    "    .collect()[0][0]\n",
    "\n",
    "taxa_mortalidade_ne = \"%.1f\" %(int(obitos_acumulado_ne) / (popTCU2019_ne / 100000))\n",
    "\n",
    "atualizacao_ne = spark.sql(\"SELECT MAX(data) FROM geral WHERE regiao ='Nordeste'\")\\\n",
    "    .collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sudeste\n",
    "casos_acumulados_se = spark.sql(\"SELECT MAX(casosAcumulado), estado FROM geral WHERE regiao ='Sudeste' GROUP BY estado\")\\\n",
    "    .groupBy().\\\n",
    "    sum().\\\n",
    "    collect()[0][0]\n",
    "\n",
    "popTCU2019_se = spark.sql(\"SELECT MAX(populacaoTCU2019), estado FROM geral WHERE regiao ='Sudeste' GROUP BY estado\")\\\n",
    "    .groupBy()\\\n",
    "    .sum()\\\n",
    "    .collect()[0][0]\n",
    "\n",
    "incidencia_se = \"%.1f\" %(int(casos_acumulados_se) / (popTCU2019_se / 100000))\n",
    "    \n",
    "obitos_acumulado_se = spark.sql(\"SELECT MAX(obitosAcumulado), estado FROM geral WHERE regiao ='Sudeste' GROUP BY estado\")\\\n",
    "    .groupBy()\\\n",
    "    .sum()\\\n",
    "    .collect()[0][0]\n",
    "\n",
    "taxa_mortalidade_se = \"%.1f\" %(int(obitos_acumulado_se) / (popTCU2019_se / 100000))\n",
    "\n",
    "atualizacao_se = spark.sql(\"SELECT MAX(data) FROM geral WHERE regiao ='Sudeste'\")\\\n",
    "    .collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "atualizacao_br = spark.sql(\"SELECT MAX(data) FROM geral WHERE regiao ='Brasil'\")\\\n",
    "    .collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t  Casos\t\tÓbitos\tIncidência/100mil hab.\t Mortalidade/100mil hab\t Atualização\n",
      "\n",
      "Brasil      \t\t 18855015 \t 526892 \t 8972.3 \t\t 250.7 \t\t 2021-07-06 00:00:00\n",
      "SUL         \t\t 3611041 \t 80705 \t\t 12046.4 \t\t 269.2 \t\t 2021-07-06 00:00:00\n",
      "Centro-Oeste\t\t 1916619 \t 49207 \t\t 11760.5 \t\t 301.9 \t\t 2021-07-06 00:00:00\n",
      "Norte       \t\t 1732862 \t 43845 \t\t 9401.9 \t\t 237.9 \t\t 2021-07-06 00:00:00\n",
      "Nordeste    \t\t 4455737 \t 107824 \t 7807.3 \t\t 188.9 \t\t 2021-07-06 00:00:00\n",
      "Sudeste     \t\t 7138803 \t 245311 \t 8078.2 \t\t 277.6 \t\t 2021-07-06 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t\\t\\t  Casos\\t\\tÓbitos\\tIncidência/100mil hab.\\t Mortalidade/100mil hab\\t Atualização\\n\")\n",
    "print(\"Brasil      \\t\\t\",casos_acumulados,\"\\t\",obitos_acumulado,\"\\t\", incidencia,\"\\t\\t\", taxa_mortalidade,\"\\t\\t\", atualizacao_br)\n",
    "print(\"SUL         \\t\\t\",casos_acumulados_s,\"\\t\",obitos_acumulado_s,\"\\t\\t\", incidencia_s,\"\\t\\t\", taxa_mortalidade_s,\"\\t\\t\", atualizacao_s)\n",
    "print(\"Centro-Oeste\\t\\t\",casos_acumulados_co,\"\\t\",obitos_acumulado_co,\"\\t\\t\", incidencia_co,\"\\t\\t\", taxa_mortalidade_co,\"\\t\\t\", atualizacao_co)\n",
    "print(\"Norte       \\t\\t\",casos_acumulados_n,\"\\t\",obitos_acumulado_n,\"\\t\\t\", incidencia_n,\"\\t\\t\", taxa_mortalidade_n,\"\\t\\t\", atualizacao_n)\n",
    "print(\"Nordeste    \\t\\t\",casos_acumulados_ne,\"\\t\",obitos_acumulado_ne,\"\\t\", incidencia_ne,\"\\t\\t\", taxa_mortalidade_ne,\"\\t\\t\", atualizacao_ne)\n",
    "print(\"Sudeste     \\t\\t\",casos_acumulados_se,\"\\t\",obitos_acumulado_se,\"\\t\", incidencia_se,\"\\t\\t\", taxa_mortalidade_se,\"\\t\\t\", atualizacao_se)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
